{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install fsspec\n!pip install gcsfs        \n!pip install git+https://github.com/qubvel/segmentation_models\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"eSLFI5W7nJSc","outputId":"6a0505db-e7b3-40f5-a451-63b80724f1c9","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sf-dl-car-classification/train.zip\n/kaggle/input/sf-dl-car-classification/sample-submission.csv\n/kaggle/input/sf-dl-car-classification/test.zip\n/kaggle/input/sf-dl-car-classification/train.csv\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (0.8.5)\nRequirement already satisfied: gcsfs in /opt/conda/lib/python3.7/site-packages (0.7.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gcsfs) (2.25.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from gcsfs) (3.7.3)\nRequirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs) (0.8.5)\nRequirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs) (1.24.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from gcsfs) (4.4.2)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs) (0.4.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (4.2.1)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (49.6.0.post20210108)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (1.15.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs) (4.7.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (5.1.0)\nRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.7.4.3)\nRequirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (1.6.3)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (3.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gcsfs) (20.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->gcsfs) (2.10)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.0.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gcsfs) (1.26.3)\nCollecting git+https://github.com/qubvel/segmentation_models\n  Cloning https://github.com/qubvel/segmentation_models to /tmp/pip-req-build-4qzagkm0\n  Running command git clone -q https://github.com/qubvel/segmentation_models /tmp/pip-req-build-4qzagkm0\n  Running command git submodule update --init --recursive -q\nCollecting keras_applications<=1.0.8,>=1.0.7\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 263 kB/s eta 0:00:01\n\u001b[?25hCollecting image-classifiers==1.0.0\n  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\nCollecting efficientnet==1.0.0\n  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation-models==1.0.1) (0.18.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->segmentation-models==1.0.1) (1.15.0)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.1.1)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.5.4)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (3.4.0)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.9.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2021.3.17)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.5)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (7.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.8.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (1.3.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models==1.0.1) (4.4.2)\nBuilding wheels for collected packages: segmentation-models\n  Building wheel for segmentation-models (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for segmentation-models: filename=segmentation_models-1.0.1-py3-none-any.whl size=33792 sha256=fb35b1caf1d36ca7e90026484f6dc68c08c14d3d26f04e566aabaffcabe392a9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-8dr8oj_c/wheels/02/cd/18/61c0bbb8766acfec68f9d20618886b7b38dfeeb95865b6ba00\nSuccessfully built segmentation-models\nInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation-models\nSuccessfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation-models-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow --upgrade #tensorflow\n\n!pip install -q efficientnet# pre-trained net\n\n!pip install git+https://github.com/mjkvaak/ImageDataAugmentor #augmentation","metadata":{"id":"5LclpQJsnJSt","outputId":"55e50fe4-4e59-4232-e40d-bdace30919b8","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.4.1)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.19.5)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.1)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.32.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.12.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.36.2)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.15.6)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20210108)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n\u001b[31mERROR: Invalid requirement: 'efficientnet#'\u001b[0m\nCollecting git+https://github.com/mjkvaak/ImageDataAugmentor\n  Cloning https://github.com/mjkvaak/ImageDataAugmentor to /tmp/pip-req-build-abdgqt2a\n  Running command git clone -q https://github.com/mjkvaak/ImageDataAugmentor /tmp/pip-req-build-abdgqt2a\nRequirement already satisfied: opencv-python>=4.2 in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (4.5.1.48)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (3.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (7.2.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.5.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.2.2)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python>=4.2->ImageDataAugmentor==0.0.0) (1.19.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (1.3.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->ImageDataAugmentor==0.0.0) (1.15.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->ImageDataAugmentor==0.0.0) (2021.1)\nBuilding wheels for collected packages: ImageDataAugmentor\n  Building wheel for ImageDataAugmentor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ImageDataAugmentor: filename=ImageDataAugmentor-0.0.0-py3-none-any.whl size=29531 sha256=78bb7a9048b8efc79ecc4c55c69086a37d086cdae5ca168d5b1e8c6ffcafa76e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-c0cj89pl/wheels/c9/bd/73/9cfa59d2393dae55bbcc30f5aa901f55fe531c66efebbc8fc3\nSuccessfully built ImageDataAugmentor\nInstalling collected packages: ImageDataAugmentor\nSuccessfully installed ImageDataAugmentor-0.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# How to train your:\n## {-- Car Identification Algorithm --}\n### (with keras, efficentnetB6, and augmentation)\n<img src=\"https://i.postimg.cc/449Smnsv/download.jpg\" width=\"750px\"/>\n","metadata":{"id":"Xf_koTmInJSt"}},{"cell_type":"markdown","source":"# Goal:\nto train a new head on EfficientnetB6 to optimize for our problem utilizing augmentation","metadata":{"id":"7GYO8KqtnJSt"}},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"bKmMv0nXnJSt"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport zipfile\nimport csv\nimport sys\nimport os\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nimport efficientnet.tfkeras as efn\n\nfrom tensorflow.keras.layers import *\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport PIL\nfrom PIL import ImageOps, ImageFilter\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\n","metadata":{"id":"OJQxQp94nJSu","trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{"id":"0ImY2XoKnJSu"}},{"cell_type":"code","source":"EPOCHS               = 10  # training epochs\nBATCH_SIZE           = 32 # optimize batches for GPU\nLR                   = 1e-3 # learning rate\nVAL_SPLIT            = 0.2 # test size\n\nCLASS_NUM            = 10  # 10 car classes\nIMG_SIZE             = 224 # image size 224x224\nIMG_CHANNELS         = 3   # RGB\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\n\nDATA_PATH = '../input/sf-dl-car-classification/'\nPATH = \"../working/cars/\"\n\n\n\nRANDOM_SEED = 69\nnp.random.seed(RANDOM_SEED)  \nPYTHONHASHSEED = 0","metadata":{"id":"lAEIgLB3nJSu","trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"id":"sjHc_v2pnJSu"}},{"cell_type":"code","source":"train_df = pd.read_csv(DATA_PATH+\"train.csv\")\nsample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\nprint(train_df.info(),\"\\n\", \"=\"*20)\nprint(train_df.head())\nprint(\"\\n\", \"=\"*20, f\"\\nCategories: {train_df.Category.nunique()} \\n\", train_df.Category.value_counts())","metadata":{"id":"12USqxSXnJSv","outputId":"f959df8a-8dd6-4d39-a5b2-925a0a193991","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 15561 entries, 0 to 15560\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Id        15561 non-null  object\n 1   Category  15561 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 243.3+ KB\nNone \n ====================\n           Id  Category\n0  100155.jpg         0\n1  100306.jpg         0\n2  100379.jpg         0\n3  100380.jpg         0\n4  100389.jpg         0\n\n ==================== \nCategories: 10 \n 1    1971\n8    1765\n6    1733\n5    1631\n0    1613\n3    1528\n2    1458\n4    1400\n9    1255\n7    1207\nName: Category, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Unloading pictures...\\n')\n# Will unzip the files so that you can see them..\nfor data_zip in ['train.zip', 'test.zip']:\n    with zipfile.ZipFile(DATA_PATH+data_zip,\"r\") as file:\n        file.extractall(PATH)\n        \nprint(os.listdir(PATH))","metadata":{"id":"omOTem3AnJSv","outputId":"d00f35b4-ccc6-43c3-ae06-9496d72d2ece","trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unloading pictures...\n\n['test_upload', 'train']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's look at the images\nplt.figure(figsize=(12,8))\n\nimage = train_df.sample(n=9)\nimg_paths = image['Id'].values\nimg_cat = image['Category'].values\n\nfor index, path in enumerate(img_paths):\n    im = PIL.Image.open(PATH+f'train/{img_cat[index]}/{path}')\n    plt.subplot(3,3, index+1)\n    plt.imshow(im)\n    plt.title('Class: '+str(img_cat[index]))\n    plt.axis('off')\nplt.show()","metadata":{"id":"tgexuF-knJSv","outputId":"bdaa1bb7-8da2-4a33-c5ed-cd5d2edb232e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like shoping on avito...\n","metadata":{"id":"dE-bUAefnJSw"}},{"cell_type":"code","source":"import random\nfolder = './cars/train/5'\na=random.choice(os.listdir(folder))\n\n\nimage = PIL.Image.open(folder+'/'+a)\nimgplot = plt.imshow(image)\nplt.show()\nprint(\"Image size: \",image.size)","metadata":{"id":"zSaugs3xnJSw","outputId":"b0ecb34e-a5d5-4a15-c1bd-c98bf6f2416a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation","metadata":{"id":"npXft88nnJSw"}},{"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose)\n\n\np=0.9\n\nAUGMENTATIONS = Compose([\n        RandomRotate90(),\n        Flip(),\n        Transpose(),\n        OneOf([\n            IAAAdditiveGaussianNoise(),\n            GaussNoise(),\n        ], p=0.2),\n        OneOf([\n            MotionBlur(p=0.2),\n            MedianBlur(blur_limit=3, p=0.1),\n            Blur(blur_limit=3, p=0.1),\n        ], p=0.2),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n        OneOf([\n            OpticalDistortion(p=0.3),\n            GridDistortion(p=0.1),\n            IAAPiecewiseAffine(p=0.3),\n        ], p=0.2),\n        OneOf([\n            CLAHE(clip_limit=2),\n            IAASharpen(),\n            IAAEmboss(),\n            RandomBrightnessContrast(),\n        ], p=0.3),\n        HueSaturationValue(p=0.3),\n    ], p=p)","metadata":{"id":"d4nNDmMXnJSw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply augmentations\nfrom ImageDataAugmentor.image_data_augmentor import *\ntrain_datagen = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        validation_split=VAL_SPLIT,\n        )\n        \ntest_datagen = ImageDataAugmentor(rescale=1./255)","metadata":{"id":"EhHIEni6nJSw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate images\ntrain_generator = train_datagen.flow_from_directory(\n    PATH+'train/',     \n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = train_datagen.flow_from_directory(\n    PATH+'train/',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data","metadata":{"id":"IdjZVXo3nJSx","outputId":"85c515a9-6ffb-43f4-fe88-5d0c3445c1a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{"id":"NK9ZEbrKnJSx"}},{"cell_type":"markdown","source":"## Base model EfficientnetB6","metadata":{"id":"oUS_HZb7nJSy"}},{"cell_type":"code","source":"base_model = efn.EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)","metadata":{"id":"Hli5y4-tnJSy","outputId":"2c4e2e67-cb47-4be6-e477-9bfe01a2c7a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's freeze the model and only train the head\nbase_model.trainable = False","metadata":{"id":"R8QkC80xnJSy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras as keras\nimport tensorflow.keras.models as M\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.callbacks as C\nfrom tensorflow.keras.preprocessing import image\n\n\nmodel=M.Sequential() # Initialize\n\nmodel.add(base_model) #base\n\n#the Head v\nmodel.add(L.GlobalAveragePooling2D(),)\nmodel.add(L.Dense(256, activation='relu'))\nmodel.add(L.BatchNormalization())\nmodel.add(L.Dropout(0.25))\nmodel.add(L.Dense(CLASS_NUM, activation='softmax'))","metadata":{"id":"Sanz6Q4ynJSy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{"id":"iVZSv2GDnJSz"}},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n\ncheckpoint = ModelCheckpoint('best_yet.hdf5' , monitor = ['val_accuracy'] , verbose = 1  , mode = 'max')\nearlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\ncallbacks_list = [checkpoint, earlystop] # callbacks for the best model and early stop if the performance doesn't improve","metadata":{"id":"DC03xGi7nJSz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n    validation_data = test_generator, \n    validation_steps = test_generator.samples//test_generator.batch_size,\n    epochs = EPOCHS,\n    callbacks = callbacks_list\n    )","metadata":{"id":"j0YoXaTenJSz","outputId":"b7753e57-e0d6-4ab4-dad0-fa63dd66c003","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate_generator(test_generator, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"id":"6_kx5_jKnJSz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('../working/model_last.hdf5')\nmodel.load_weights('best_yet.hdf5')","metadata":{"id":"Helfep5ynJSz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning","metadata":{"id":"qTNdvo-YnJSz"}},{"cell_type":"code","source":"print(\"Number of layers in the base model: \", len(base_model.layers))\nbase_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = len(base_model.layers)//2\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False\nprint(\"Number of trainable variables\", len(base_model.trainable_variables))","metadata":{"id":"VTtzaD8fnJS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS               = 10\nBATCH_SIZE           = 16 # decreasing batch size\nLR                   = 1e-4 #increasing LR to fine tune even finer","metadata":{"id":"wBd92eN6nJS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same as before\ntrain_generator = train_datagen.flow_from_directory(\n    PATH+'train/',    \n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = train_datagen.flow_from_directory(\n    PATH+'train/',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n\n  \ncheckpoint = ModelCheckpoint('best_yet.hdf5' , monitor = ['accuracy'] , verbose = 1  , mode = 'max')\nearlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\ncallbacks_list = [checkpoint, earlystop]\n\n# Training\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n    validation_data = test_generator, \n    validation_steps = test_generator.samples//test_generator.batch_size,\n    epochs = EPOCHS,\n    callbacks = callbacks_list\n    )","metadata":{"id":"T0Cz-XFWnJS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate_generator(test_generator, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"id":"EKU3nbKHnJS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('../working/model_last.hdf5')\nmodel.load_weights('best_yet.hdf5')","metadata":{"id":"7kC0KnbVnJS0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now unfreeze the whole model","metadata":{"id":"k2GALUJbnJS1"}},{"cell_type":"code","source":"base_model.trainable = True\nEPOCHS               = 8  \nBATCH_SIZE           = 8 #even smaller\nLR                   = 1e-5 # even larger","metadata":{"id":"TvKV1vswnJS1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    PATH+'train/',     \n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = train_datagen.flow_from_directory(\n    PATH+'train/',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n\n  \ncheckpoint = ModelCheckpoint('best_yet.hdf5' , monitor = ['accuracy'] , verbose = 1  , mode = 'max')\nearlystop = EarlyStopping(monitor='accuracy', patience=5, restore_best_weights=True)\ncallbacks_list = [checkpoint, earlystop]\n\n#Training\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n    validation_data = test_generator, \n    validation_steps = test_generator.samples//test_generator.batch_size,\n    epochs = EPOCHS,\n    callbacks = callbacks_list\n    )","metadata":{"id":"-rw7gvUgnJS1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate_generator(test_generator, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"id":"Lc5q4-HOnJS1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('../working/model_last.hdf5')\nmodel.load_weights('best_yet.hdf5')","metadata":{"id":"LSldFUiAnJS1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction on Test","metadata":{"id":"E3lLFTgonJS1"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ntest_sub_generator = test_datagen.flow_from_dataframe( \n    dataframe=sample_submission,\n    directory=PATH+'test_upload/',\n    x_col=\"Id\",\n    y_col=None,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,)\n","metadata":{"id":"SYluijK8nJS1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sub_generator.reset()\npredictions = model.predict_generator(test_sub_generator, steps=len(test_sub_generator), verbose=1) \npredictions = np.argmax(predictions, axis=-1) #multiple categories\nlabel_map = (train_generator.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]","metadata":{"id":"fj5CNZL3nJS2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"../working/cars\")","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"filenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\nsubmission['Id'] = submission['Id'].replace('test_upload/','')\nsubmission.to_csv('submission.csv', index=False)\nprint('Save submit')","metadata":{"id":"eZzrELaLnJS2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}